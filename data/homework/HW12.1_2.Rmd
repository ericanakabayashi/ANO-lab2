---
title: "Unit 12 HW: The Classical Linear Model"
output: 'pdf_document'  
classoption: landscape
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(patchwork)
library(car)
library(lmtest)
library(sandwich)
library(stargazer)
```

```{r chunk options, include=FALSE}
knitr::opts_chunk$set(message=FALSE, dpi=300)
```


```{r get robust ses, include=FALSE }
rse <- function(model) { 
  sqrt(diag(vcovHC(model)))
  }
```

#Loading in the data

```{r load-data, message=FALSE}

videos <- read_delim("videos.txt", delim = "\t")
videosfilteredout <- videos %>% filter(!is.na(videos$views))

glimpse(videosfilteredout)
summary(videosfilteredout)
```


#creating the model

```{r}
#model_one <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)
videos_model <-lm(log(views) ~ rate + length, data=videosfilteredout)

videos_modelnotfitted <-lm(views ~ rate + length, data=videosfilteredout)


```

- Q1.1 I.I.D. data

>To assess IID data, we need to understand the sampling process used to collect the data. From the videos.txt documentation, we learned that the videos were selected initially from the set of videos included in "Recently Featured", "Most Viewed", "Top Rated", and "Most Discussed" from "Today", "This Week", "This Month" and All Time" on February 22nd, 2007. This totalled 189 unique videos followed by a "crawl" and this process was followed for the remaining data collected through 2008. The data collected was by video ID and includes the variables uploader, age, category, length, views, rate, ratings, comments, and related IDs extracted from the YouTube API. There are several reasons why this data collection process might not result in IID data, below.  

> The primary focus of this data collection was on successful videos by collecting from the "Recently Featured", "Most Viewd", "Top Rated", and "Most Discussed" categories. This likely indicates that the data is not distributed as the population of all YouTube videos and is heavily weighted towards successful videos.  

> Clustering may also be a factor in this data given that videos in the top categories are likely related to one another. They could be videos uploaded by the same user or very similar content.  

>Given the findings above, we believe the IID assumption 

- Q1.2 No Perfect Collinearity

> To evaluate collinearity, we can look at our coefficients, and notice that R has not dropped any variables. 

```{r}
videos_model$coefficients
```

> This tells us that there is no perfect collinearity between our variables. Perfect collinearity indicates that one data series can be exactly produced through a simple transformation from another. Intuitively, this also makes sense because you wouldn't expect the length of a video to be transformable to rate of a video and vice-versa.  

>This assumption also includes the requirement that a BLP exists, which may not happen if there are heavy tails.  In this case, though, we don't see any distributions that look like they have unusually low or high values.  

>Given the findings above, we believe the IID assumption is not met. 
 
1.3

- Linear conditional expectation

> To assess whether there is a linear conditional expectation, we've learned to look at the predicted vs. residuals of the model.

```{r}

#videos_model <-lm(log(views) ~ rate + length, data=videosfilteredout)

videosfilteredout %>% 
  mutate(
    model_one_preds = predict(videos_model), 
    model_one_resids = resid(videos_model)
  ) %>% 
  ggplot(aes(model_one_preds, model_one_resids)) + 
  geom_point() + 
  stat_smooth()

videosfilteredout %>% 
  mutate(
    model_one_preds = predict(videos_model), 
    model_one_resids = resid(videos_model)
  ) %>% 
  ggplot(aes(model_one_preds, log(videosfilteredout$views))) + 
  geom_point() + 
  stat_smooth()

```
```{r}

# library
library(ggplot2)


#videos_model <-lm(log(views) ~ rate + length, data=videosfilteredout)
 


ggplot(videosfilteredout, aes(x=videosfilteredout$rate, y=videosfilteredout$views)) + 
    geom_point()

ggplot(videosfilteredout, aes(x=videosfilteredout$length, y=videosfilteredout$views)) + 
    geom_point()


```

On the far out right side of the Residual vs. Estimators graph above, it's possible to note that a non-linear relationship in this data, and on the far right end of the data our model seem to be producing underestimated data. In order to correct this, one could try variable transformations, hoping to end up with a more linear pattern in this plot. We would say that the requirement was no met.

1.4 Homoskedastic Error

```{r}
plot(videos_model, which=3)
```
To assess whether the distribution of the errors is homoskedastic, we can examine the residuals versus fitted plot again.  By looking at the plot above, it does look like there might be some inbalance in the variance of the residuals at the upper side of the predicted values, but it is not severe, and could be explained by the rate being equal or more the zero. Over all it looks like the Homoskedastic Error could be assumed.



